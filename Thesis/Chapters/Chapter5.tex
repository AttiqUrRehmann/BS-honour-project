\chapter{Conclusion}
\lhead{Chapter 5. \emph{Conclusion}}


In high-dimensional data where sample size is smaller than the number of variables, the sample estimator of the covariance matrix is not invertible and contains a large amount of estimation error. In these situations the classical multivariate techniques, which rely on the covariance matrix or its inverse, either fails to work or becomes unreliable (when $n$ is larger but comparable to $p$). To overcome this problem various methods have been proposed in the literature to improve upon the sample estimator such as Moore-Penrose generalized inverse and other shrinkage estimation methods. 

In this study a new regularized method of the covariance matrix is developed, which is also like shrinkage estimation is the linear convex combination of the sample covariance matrix and a target matrix. This new regularized estimator depends on the penalty parameter whose value need to be chosen in the appropriate range of values. The value of the penalty parameter is achieved by maximizing the log-likelihood function of the multivariate normal distribution. The proposed estimator is not only invertible but also well-conditioned. Furthermore, two new more informative targets have been used to shrink the sample covariance matrix towards them. These targets are the AR(1) and exchangeable covariance structures, which depends on the correlation parameter and need to be estimated in the appropriate range of values. To choose an appropriate value of the correlation parameter maximum log-likelihood function of the multivariate normal distribution is used. 

The behaviour of the proposed method compare to the shrinkage method is explored through large simulations. The simulation experiments show that the proposed estimator perform better than the shrinkage estimator whenever the target is correctly specified in case of AR(1) and exchangeable covariance structures. However, its performance becomes slightly weaker than the shrinkage estimator when the target is incorrectly specified as AR(1) or exchangeable while the true covariance matrix is identity matrix. In case of random covariance structure our proposed estimator perform better than the shrinkage estimator.

It is worth noticing that the proposed estimator is also analytically much simpler and computationally inexpensive procedure compare to the shrinkage method.
